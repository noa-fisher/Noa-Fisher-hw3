{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EigenFaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin make sure you read the pdf document fully and that you did not change the order of folders or files. Keep the same order as in the zip file of HW3. For this assignment by the way, there is no need to divide the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use(['ggplot']) \n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading a dataset of human faces out of `scikit-learn` datasets. If for some reason the next cell fails, you may activate the next one (which is now in comments). Make sure that the `X.npy` file is at the same location of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "lfw_dataset = fetch_lfw_people(min_faces_per_person=0)\n",
    "_, h, w = lfw_dataset.images.shape\n",
    "X = lfw_dataset.data\n",
    "\n",
    "y = lfw_dataset.target\n",
    "target_names = lfw_dataset.target_names\n",
    "target_names = target_names[y]\n",
    "print(\"Dataset images are at the shape of {}X{}\".format(h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.load(''Data/X.npy')\n",
    "# h = 62\n",
    "# w = 47\n",
    "# print(\"Dataset images are at the shape of {}X{}\".format(h,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13,233 images were flattened and stacked in the matrix $X$ so every row is an image and every column is a pixel. We can see that by the shape of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some of the images of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images,target_names, h, w,rows=3, cols=4):\n",
    "    plt.figure(figsize=(14,14))\n",
    "    for i in range(rows * cols):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(images[i, :].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(target_names[i])\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(X, target_names, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use the `PCA` package, we have to center our data (even though `pca.fit_transform()` does it automatically, here we will do it for practicing). The center of our data ($\\mu$) is the mean of each pixel along all of our images. Thus, the center has 2,914 elements where the first element is the mean of all of the images' \"first\" pixels, the second is the mean of all of the images' \"second\" pixels etc. Notice that the images are already flattened in $X$ which might help you.\n",
    "\n",
    "Name your `pca` object: `pca`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $\\mu$ of $X$ and center every image in $X$ (row vector) according to $\\mu$. Keep $X$ with same name `X`. Name the $\\mu$ vector `mu_orig`.\n",
    "\n",
    "If you can, apply centering without a single loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------Implement your code here-----------------------\n",
    "\n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a PCA model on $X$ (**without whitening**) that would preserve 99.5% of the energy. Find out how many eigenvectors you remained with and set it as $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------Implement your code here-----------------------\n",
    "\n",
    "K = \n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\">***Question:***</span> *If we now have $K$ eigenvectors, by how many dimensions we have reduced our data?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\">***Answer:***</span> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see these eigenfaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eigenfaces(eigenvec_mat, h, w, rows=3, cols=4):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(rows * cols):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(eigenvec_mat[i, :].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(\"$u_{\" + str(i+1) + \"}$\")\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eigenvec_mat` is the matrix of the eigenvectors calculated where every row is an eigenvector and the first row has the highest eigenvalue and the second row has the next highest eigenvalue etc. Think where this matrix was calculated in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvec_mat = \n",
    "plot_eigenfaces(eigenvec_mat, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got it right, you should see those \"ghosts\" we talked about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's move on towards the more interesting part! We will start by showing your resized gray level face image. It might be a bit distorted due to resizing. Change the format of `.jpg` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_format = '.jpg' # change the format according to your image. Do not delete the dot sign before the format name\n",
    "image = Image.open('Data/Orig' + img_format)\n",
    "gray = image.convert('L')\n",
    "g = gray.resize((w, h))\n",
    "orig = np.asarray(g).astype('float32')\n",
    "plt.imshow(orig, cmap=plt.cm.gray)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should flatten our image and center it by the same $\\mu$ vector you calculated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_img = np.asarray(g).astype('float32').flatten()\n",
    "flattened_img -= mu_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `U` to be the matrix containing the first K eigenvectors (rows) extracted from `eigenvec_mat`. Now, calculate the projections $c_i$ so you would have a vector with $K$ elements where the first element is $c_1$ and the second is $c_2$ etc. Relate to the pdf if you are not sure. **Note:** you can only use `U`, your flattened imgae and `numpy` for this section. **Do not apply `pca` methods for calculating the projections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------Implement your code here-----------------------\n",
    "\n",
    "#--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got it all correct, then the cell below will show you how your face is reconstructed as a linear combination of the eigenfaces which actually means that your face is a linear combination of other people's faces! The coefficients will appear with their adequate sign in the title in the left and the image constructed will appear slowly on the right.\n",
    "\n",
    "Notice the \"correction\" of the centering made in the loop for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.zeros((h*w,))\n",
    "fig, axes = plt.subplots(1, 2,figsize=(8,8))\n",
    "for j in range(K):\n",
    "    s += c[j]*U[j, :]\n",
    "    if np.mod(j, 10) == 0:\n",
    "        axes[0].imshow(U[j, :].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        axes[0].grid(False)\n",
    "        corrected_image = s + mu_orig\n",
    "        axes[1].imshow(corrected_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "        axes[1].grid(False)\n",
    "        if c[j] < 0:\n",
    "            axes[0].set_title('{:.2f}'.format(c[j]))\n",
    "        else:\n",
    "            axes[0].set_title('+{:.2f}'.format(c[j]))\n",
    "        display(fig)\n",
    "        clear_output(wait = True)\n",
    "        plt.pause(0.3)\n",
    "        if c[j] > 0:\n",
    "            p = '+' + str(c[j])\n",
    "        else:\n",
    "            p = str(c[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(orig, cmap=plt.cm.gray) \n",
    "axes[0].title.set_text('Original')\n",
    "axes[0].grid(False)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "axes[1].imshow(corrected_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "axes[1].title.set_text('Reconstructed ')\n",
    "axes[1].grid(False)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you got a nice reconstruction from those \"ghosts\". Now, we will continue to the face recgonition part. \n",
    "\n",
    "Facebook has also learned the natural basis of human faces using a database of different faces much like you did here. Now, some of your images are tagged (labeled) in a larger (mostly different) database with your name. It then projects the images on the PCA axes and gets an \"id\" ($c$ coefficients) for each and every image in the tagged database. When someone uploades a picture without tagging, Facebook uses the same learned PCA in order to create this new image an id . and then looks through the tagged id database for comparison. The closest match (user) will get a notification whether or not he\\she would like to be tagged according to the match.\n",
    "\n",
    "We will make a simple simulation. First we will create a tagged database, which in our case will simply be `X` concatenated with your \"Orig\" flattened and centered image. We will call it `X_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X.copy()\n",
    "X_new = np.vstack([X_new, flattened_img])\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of rows was increased by 1 accordingly. Now we will add your name. Fill your name as a string below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \n",
    "target_new = target_names.copy()\n",
    "target_new = np.append(target_names, name)\n",
    "target_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle the databse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_new, y_new = shuffle(X_new, target_new, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's simulate a new uploaded untagged (out of the database) image and show it. You can use either \"tilt1\" or \"tilt2\". Then we will also center it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_format = '.jpg' # change the format according to your image. Do not delete the dot sign before the format name\n",
    "image = Image.open('Data/tilt1' + img_format)  #change the name if needed\n",
    "gray = image.convert('L')\n",
    "g = gray.resize((w, h))\n",
    "new_input = np.asarray(g).astype('float32')\n",
    "plt.imshow(new_input, cmap=plt.cm.gray)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_img_new = np.asarray(g).astype('float32').flatten()\n",
    "flattened_img_new -= mu_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the recognition part:\n",
    "First, project the new database onto the **already fitted** PCA basis. You can use `pca` methods. Then calculate the projections (`c` vector) for the new input similarly to what you did before. Then run in a for loop and measure the Euclidean distance between the new output projection to each and everyone of the projections of the new database. Finally return the index (row number) of the best matching image in the database (minimal distance) and name it as `idx_match`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------Implement your code here-----------------------\n",
    "\n",
    "#--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you got it right. If so, on left image you would see your tilted untagged image and on the right the best matching image of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(7,7))\n",
    "axes[0].imshow(new_input, cmap=plt.cm.gray) \n",
    "axes[0].title.set_text('X')\n",
    "axes[0].grid(False)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "axes[1].imshow((X_new[idx_match, :] + mu_orig).reshape((62, 47)), cmap=plt.cm.gray)\n",
    "axes[1].title.set_text('X was recognized as {} '.format(y_new[idx_match]))\n",
    "axes[1].grid(False)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of PCA is the ability to find your image across 13,324 images with using only Euclidien distance of $K$ elements vectors rather using image matching techniques involving higher dimensional data, computational power and mostly morphlogical priors and filters! Try it also for the second image and check if it worked.\n",
    "\n",
    "Notice that even if it failed to recognize you but the code was correct, you will get the full credit points for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you enjoyed :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Basically, once you run all of the Jupyter cells and save the file, then it would be saved with all of your outputs. Thus, only the pdf with your answers for the theoretical part and the notebook with the outputs should be uploaded to GitHub without your personal jpg images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
